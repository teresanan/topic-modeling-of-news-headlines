{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling on News Headlines\n",
    "\n",
    "#### Dataset:\n",
    "\n",
    "The dataset is from Kaggle which includes over 1 million news headline published over a period of 17 years.\n",
    "https://www.kaggle.com/therohk/million-headlines\n",
    "\n",
    "#### Problem Statement:\n",
    "\n",
    "Our goal in this analysis is to find the topics of the news headlines and make predictions on unseen documents using Gensim LDA model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit\n",
       "3      20030219           air nz staff in aust strike for pay rise\n",
       "4      20030219      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('abcnews-date-text.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1186018"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1,186,018 records which is too many for this practice analysis, therefore, we will take a random sample of 10,000 records from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = data.sample(10000, random_state=42)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090902</td>\n",
       "      <td>extended interview terence higgins speaks with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20100329</td>\n",
       "      <td>council ceo temporarily replaced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20160410</td>\n",
       "      <td>adelaide united finally breaks a league premie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20090921</td>\n",
       "      <td>rocca hangs up the boots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20071114</td>\n",
       "      <td>syphilis threatens rare marsupial with extinction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20090902  extended interview terence higgins speaks with...\n",
       "1      20100329                   council ceo temporarily replaced\n",
       "2      20160410  adelaide united finally breaks a league premie...\n",
       "3      20090921                           rocca hangs up the boots\n",
       "4      20071114  syphilis threatens rare marsupial with extinction"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = documents.reset_index(drop=True)\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the Text\n",
    "\n",
    "In this step, we are going to turn the text into a bag-of-words representation, then use lemmatize and stemming techniques to reduce the inflectional forms of each word into a common base or root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(doc):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(doc, pos='v'))\n",
    "\n",
    "def text_preprocess(doc):\n",
    "    # min_len and max_len: minimum and maximum lengths of token (inclusive)\n",
    "    processed_words = gensim.utils.simple_preprocess(doc,min_len=3, max_len=15)  \n",
    "    stop_words = gensim.parsing.preprocessing.STOPWORDS\n",
    "    tokens = [lemmatize_stemming(token) for token in processed_words if token not in stop_words]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "council awaits temple tourism decision\n",
      "\n",
      "['council', 'await', 'templ', 'tourism', 'decis']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents.iloc[551,1]  \n",
    "print(doc_sample)\n",
    "print()\n",
    "print(text_preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents['headline_text'].apply(text_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Corpus to Vectors \n",
    "There are 2 ways of transforming corpus to word vectors. One is Bag-of-words representation which uses word count, so the word order does not matter. Another way is tf-idf which transforms Bag-of-words word count into word weight. First, we will try Bag-of-words approach.\n",
    "\n",
    "#### 1. Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary - a mapping between words and their integer ids\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)  # this dictionary is not the regular Python dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the first 10 words in the dictionary. Note the dictionary consists of the word ID and the word pairs. The word ID is created for this specific corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 extend\n",
      "1 higgin\n",
      "2 interview\n",
      "3 speak\n",
      "4 terenc\n",
      "5 ceo\n",
      "6 council\n",
      "7 replac\n",
      "8 temporarili\n",
      "9 adelaid\n",
      "10 break\n",
      "11 drought\n"
     ]
    }
   ],
   "source": [
    "# print out the first 10 words in the dictionary\n",
    "\n",
    "for i, (k, v) in enumerate(dictionary.iteritems()):\n",
    "    print(k, v)\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to filter the dictionary with some parameters.The `filter_extremes` method filters out tokens in the dictionary by their frequency.\n",
    "\n",
    "- no_below (int, optional) – Keep tokens which are contained in at least no_below documents.\n",
    "- no_above (float, optional) – Keep tokens which are contained in no more than no_above documents (fraction of total corpus size, not an absolute number).\n",
    "- keep_n (int, optional) – Keep only the first keep_n most frequent tokens.\n",
    "- keep_tokens (iterable of str) – Iterable of tokens that must stay in dictionary after filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.6, keep_n=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "doc2bow converts document into the bag-of-words (BoW) format, which is a list of (token_id, token_count) tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 1), (21, 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output means: in document 8, the token_id 7 appears 1 time, token_id 21 appears 1 time. If we want to know which word the token_id corresponds to, we will need to map it out from the pre-defined dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 drought 1\n",
      "21 hit 1\n"
     ]
    }
   ],
   "source": [
    "# let's print out an example\n",
    "\n",
    "bow_doc_8 = bow_corpus[8]\n",
    "for id, count in bow_doc_8:\n",
    "    print(id, dictionary[id], count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned earlier, Bag-of-words simply uses word count frequency to create vectors. Next, we will try the second appraoch, tf-idf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x1efec18d438>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform bow_corpus to word and weight\n",
    "\n",
    "corpus_tfidf = tfidf[bow_corpus]  # different syntax than Scikit-Learn,it uses []\n",
    "corpus_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 drought 0.7366323814482965\n",
      "21 hit 0.6762933790906216\n"
     ]
    }
   ],
   "source": [
    "# print out the same example - document 8\n",
    "\n",
    "corpus_tfidf_8 = corpus_tfidf[8]\n",
    "for id, count in corpus_tfidf_8:\n",
    "    print(id, dictionary[id], count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how tf-idf transforms the word count to word weight. Previously, these words all had the same word which 1, but they have different weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling with LDA\n",
    "\n",
    "#### 1. Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=20, id2word=dictionary, passes=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic0:\n",
      "0.063*\"chang\" + 0.052*\"flood\" + 0.050*\"australia\" + 0.049*\"work\" + 0.047*\"trial\" + 0.042*\"elect\" + 0.035*\"murder\" + 0.032*\"futur\" + 0.029*\"campaign\" + 0.024*\"tour\"\n",
      "Topic1:\n",
      "0.046*\"time\" + 0.044*\"return\" + 0.044*\"storm\" + 0.041*\"rise\" + 0.039*\"train\" + 0.039*\"leav\" + 0.029*\"die\" + 0.028*\"push\" + 0.028*\"grant\" + 0.028*\"parliament\"\n",
      "Topic2:\n",
      "0.055*\"coast\" + 0.043*\"north\" + 0.043*\"fight\" + 0.042*\"gold\" + 0.035*\"break\" + 0.032*\"road\" + 0.030*\"west\" + 0.029*\"victim\" + 0.027*\"investig\" + 0.025*\"east\"\n",
      "Topic3:\n",
      "0.103*\"new\" + 0.074*\"nsw\" + 0.068*\"australian\" + 0.059*\"day\" + 0.049*\"test\" + 0.048*\"hospit\" + 0.042*\"miss\" + 0.032*\"region\" + 0.029*\"tax\" + 0.022*\"chief\"\n",
      "Topic4:\n",
      "0.056*\"end\" + 0.039*\"queensland\" + 0.038*\"review\" + 0.037*\"join\" + 0.036*\"green\" + 0.036*\"women\" + 0.035*\"polici\" + 0.031*\"discuss\" + 0.030*\"remain\" + 0.027*\"blue\"\n",
      "Topic5:\n",
      "0.094*\"council\" + 0.043*\"union\" + 0.030*\"reject\" + 0.029*\"look\" + 0.026*\"free\" + 0.025*\"consid\" + 0.023*\"season\" + 0.021*\"aussi\" + 0.021*\"trade\" + 0.021*\"decis\"\n",
      "Topic6:\n",
      "0.119*\"say\" + 0.094*\"plan\" + 0.033*\"cut\" + 0.033*\"research\" + 0.026*\"state\" + 0.025*\"mayor\" + 0.021*\"second\" + 0.019*\"concern\" + 0.019*\"countri\" + 0.019*\"approv\"\n",
      "Topic7:\n",
      "0.074*\"face\" + 0.049*\"lead\" + 0.044*\"court\" + 0.044*\"dead\" + 0.040*\"law\" + 0.038*\"opposit\" + 0.037*\"dog\" + 0.032*\"shoot\" + 0.027*\"plane\" + 0.027*\"liber\"\n",
      "Topic8:\n",
      "0.063*\"report\" + 0.062*\"open\" + 0.055*\"school\" + 0.043*\"nation\" + 0.029*\"close\" + 0.026*\"news\" + 0.025*\"abc\" + 0.025*\"talk\" + 0.025*\"share\" + 0.023*\"high\"\n",
      "Topic9:\n",
      "0.064*\"urg\" + 0.049*\"boost\" + 0.046*\"farmer\" + 0.032*\"weather\" + 0.031*\"famili\" + 0.029*\"health\" + 0.028*\"farm\" + 0.025*\"welcom\" + 0.025*\"tasmanian\" + 0.023*\"clash\"\n",
      "Topic10:\n",
      "0.081*\"warn\" + 0.076*\"hous\" + 0.045*\"fear\" + 0.040*\"price\" + 0.031*\"public\" + 0.030*\"canberra\" + 0.027*\"threaten\" + 0.027*\"john\" + 0.024*\"sell\" + 0.023*\"develop\"\n",
      "Topic11:\n",
      "0.098*\"win\" + 0.043*\"world\" + 0.043*\"cup\" + 0.033*\"final\" + 0.024*\"unit\" + 0.023*\"india\" + 0.023*\"build\" + 0.023*\"risk\" + 0.022*\"tiger\" + 0.021*\"australia\"\n",
      "Topic12:\n",
      "0.103*\"interview\" + 0.065*\"sydney\" + 0.049*\"take\" + 0.038*\"indigen\" + 0.037*\"cost\" + 0.031*\"lose\" + 0.030*\"question\" + 0.024*\"accus\" + 0.024*\"claim\" + 0.023*\"line\"\n",
      "Topic13:\n",
      "0.141*\"man\" + 0.069*\"charg\" + 0.059*\"attack\" + 0.050*\"crash\" + 0.048*\"car\" + 0.041*\"hit\" + 0.028*\"murder\" + 0.027*\"bushfir\" + 0.018*\"babi\" + 0.018*\"woman\"\n",
      "Topic14:\n",
      "0.059*\"fund\" + 0.049*\"arrest\" + 0.038*\"offer\" + 0.031*\"get\" + 0.030*\"drug\" + 0.030*\"assault\" + 0.028*\"offic\" + 0.028*\"worker\" + 0.027*\"rescu\" + 0.025*\"man\"\n",
      "Topic15:\n",
      "0.201*\"polic\" + 0.049*\"set\" + 0.041*\"feder\" + 0.039*\"alleg\" + 0.038*\"defend\" + 0.036*\"brisban\" + 0.027*\"budget\" + 0.022*\"sign\" + 0.021*\"clear\" + 0.020*\"drug\"\n",
      "Topic16:\n",
      "0.048*\"ban\" + 0.041*\"fall\" + 0.035*\"home\" + 0.034*\"start\" + 0.033*\"labor\" + 0.025*\"prison\" + 0.025*\"right\" + 0.024*\"troop\" + 0.023*\"ahead\" + 0.023*\"game\"\n",
      "Topic17:\n",
      "0.081*\"govt\" + 0.052*\"govern\" + 0.039*\"group\" + 0.038*\"protest\" + 0.033*\"tell\" + 0.031*\"leader\" + 0.029*\"market\" + 0.027*\"south\" + 0.027*\"vote\" + 0.024*\"inquiri\"\n",
      "Topic18:\n",
      "0.077*\"death\" + 0.047*\"minist\" + 0.040*\"china\" + 0.036*\"probe\" + 0.036*\"support\" + 0.035*\"seek\" + 0.034*\"act\" + 0.028*\"mine\" + 0.026*\"speak\" + 0.026*\"busi\"\n",
      "Topic19:\n",
      "0.060*\"year\" + 0.057*\"jail\" + 0.052*\"kill\" + 0.050*\"help\" + 0.041*\"appeal\" + 0.033*\"hold\" + 0.031*\"child\" + 0.030*\"adelaid\" + 0.030*\"sentenc\" + 0.029*\"strike\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics():\n",
    "    print(f'Topic{idx}:')\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 10,000 news headlines, there should be more than 20 topics. But this exercise, we will limit to 20 topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=20, id2word=dictionary, passes=10,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`print_topics_` method by default only prints out the first 20 topics, if we specified more topics and we need to print out them all, pass `-1`as an argument will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic0:\n",
      "0.054*\"take\" + 0.051*\"work\" + 0.047*\"flood\" + 0.038*\"cost\" + 0.038*\"elect\" + 0.035*\"futur\" + 0.035*\"tour\" + 0.026*\"award\" + 0.024*\"tas\" + 0.023*\"point\"\n",
      "Topic1:\n",
      "0.043*\"return\" + 0.041*\"storm\" + 0.037*\"drought\" + 0.036*\"time\" + 0.029*\"rat\" + 0.026*\"leav\" + 0.026*\"parliament\" + 0.025*\"beat\" + 0.024*\"green\" + 0.023*\"presid\"\n",
      "Topic2:\n",
      "0.037*\"coast\" + 0.034*\"break\" + 0.032*\"gold\" + 0.027*\"road\" + 0.026*\"north\" + 0.025*\"investig\" + 0.025*\"fight\" + 0.022*\"train\" + 0.022*\"victim\" + 0.021*\"war\"\n",
      "Topic3:\n",
      "0.069*\"australian\" + 0.051*\"miss\" + 0.047*\"test\" + 0.045*\"day\" + 0.038*\"tax\" + 0.037*\"hospit\" + 0.035*\"fall\" + 0.035*\"seek\" + 0.028*\"afl\" + 0.027*\"new\"\n",
      "Topic4:\n",
      "0.051*\"end\" + 0.039*\"review\" + 0.036*\"join\" + 0.031*\"women\" + 0.031*\"discuss\" + 0.028*\"remain\" + 0.027*\"olymp\" + 0.025*\"name\" + 0.025*\"cricket\" + 0.025*\"gas\"\n",
      "Topic5:\n",
      "0.035*\"aussi\" + 0.034*\"want\" + 0.032*\"free\" + 0.029*\"hear\" + 0.028*\"decis\" + 0.028*\"union\" + 0.027*\"releas\" + 0.027*\"trade\" + 0.025*\"council\" + 0.025*\"look\"\n",
      "Topic6:\n",
      "0.039*\"say\" + 0.034*\"research\" + 0.032*\"countri\" + 0.032*\"plan\" + 0.028*\"hour\" + 0.026*\"concern\" + 0.026*\"mayor\" + 0.025*\"melbourn\" + 0.024*\"second\" + 0.024*\"state\"\n",
      "Topic7:\n",
      "0.045*\"face\" + 0.043*\"lead\" + 0.035*\"shoot\" + 0.032*\"dead\" + 0.032*\"law\" + 0.031*\"court\" + 0.030*\"final\" + 0.029*\"dog\" + 0.028*\"appeal\" + 0.025*\"plane\"\n",
      "Topic8:\n",
      "0.042*\"open\" + 0.037*\"nation\" + 0.036*\"report\" + 0.030*\"close\" + 0.030*\"ahead\" + 0.028*\"abc\" + 0.028*\"sale\" + 0.026*\"news\" + 0.025*\"consid\" + 0.024*\"talk\"\n",
      "Topic9:\n",
      "0.034*\"farmer\" + 0.032*\"minist\" + 0.031*\"boost\" + 0.030*\"act\" + 0.027*\"urg\" + 0.026*\"weather\" + 0.023*\"announc\" + 0.023*\"clash\" + 0.022*\"famili\" + 0.019*\"tasmanian\"\n",
      "Topic10:\n",
      "0.062*\"hous\" + 0.044*\"warn\" + 0.041*\"price\" + 0.034*\"rule\" + 0.033*\"public\" + 0.033*\"john\" + 0.028*\"threaten\" + 0.028*\"industri\" + 0.027*\"sell\" + 0.024*\"develop\"\n",
      "Topic11:\n",
      "0.082*\"win\" + 0.037*\"rise\" + 0.034*\"adelaid\" + 0.029*\"rural\" + 0.029*\"unit\" + 0.028*\"chang\" + 0.027*\"build\" + 0.026*\"rate\" + 0.025*\"climat\" + 0.025*\"spark\"\n",
      "Topic12:\n",
      "0.147*\"interview\" + 0.034*\"lose\" + 0.033*\"sydney\" + 0.032*\"servic\" + 0.029*\"question\" + 0.027*\"indigen\" + 0.024*\"push\" + 0.024*\"line\" + 0.022*\"back\" + 0.022*\"titl\"\n",
      "Topic13:\n",
      "0.059*\"man\" + 0.056*\"charg\" + 0.046*\"attack\" + 0.045*\"car\" + 0.041*\"crash\" + 0.036*\"murder\" + 0.031*\"woman\" + 0.028*\"bushfir\" + 0.026*\"continu\" + 0.023*\"polic\"\n",
      "Topic14:\n",
      "0.039*\"arrest\" + 0.034*\"fund\" + 0.030*\"rescu\" + 0.030*\"worker\" + 0.029*\"offic\" + 0.028*\"offer\" + 0.024*\"problem\" + 0.023*\"earli\" + 0.022*\"men\" + 0.021*\"protect\"\n",
      "Topic15:\n",
      "0.053*\"set\" + 0.046*\"trial\" + 0.042*\"feder\" + 0.038*\"defend\" + 0.036*\"brisban\" + 0.034*\"alleg\" + 0.031*\"budget\" + 0.030*\"farm\" + 0.029*\"sign\" + 0.028*\"murray\"\n",
      "Topic16:\n",
      "0.039*\"cup\" + 0.035*\"world\" + 0.029*\"ban\" + 0.028*\"guilti\" + 0.026*\"start\" + 0.026*\"run\" + 0.026*\"canberra\" + 0.025*\"life\" + 0.021*\"right\" + 0.021*\"meet\"\n",
      "Topic17:\n",
      "0.037*\"market\" + 0.033*\"group\" + 0.029*\"tell\" + 0.029*\"protest\" + 0.028*\"kill\" + 0.028*\"centr\" + 0.026*\"prison\" + 0.026*\"south\" + 0.024*\"vote\" + 0.022*\"govt\"\n",
      "Topic18:\n",
      "0.045*\"death\" + 0.040*\"china\" + 0.036*\"sentenc\" + 0.035*\"speak\" + 0.032*\"probe\" + 0.027*\"mine\" + 0.027*\"rain\" + 0.024*\"busi\" + 0.024*\"drop\" + 0.023*\"way\"\n",
      "Topic19:\n",
      "0.050*\"jail\" + 0.049*\"year\" + 0.032*\"hold\" + 0.030*\"injur\" + 0.028*\"teen\" + 0.028*\"abus\" + 0.028*\"drug\" + 0.027*\"strike\" + 0.027*\"fire\" + 0.027*\"show\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics():\n",
    "    print(f'Topic{idx}:')\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "####  1.Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'drought hits us wheat forecast'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's use our favoriate sample - document 8\n",
    "\n",
    "documents.iloc[8,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13, 0.6833238005638123, 0.141*\"man\" + 0.069*\"charg\" + 0.059*\"attack\" + 0.050*\"crash\" + 0.048*\"car\"\n",
      "3, 0.01666717417538166, 0.103*\"new\" + 0.074*\"nsw\" + 0.068*\"australian\" + 0.059*\"day\" + 0.049*\"test\"\n",
      "15, 0.016667170450091362, 0.201*\"polic\" + 0.049*\"set\" + 0.041*\"feder\" + 0.039*\"alleg\" + 0.038*\"defend\"\n",
      "0, 0.016667168587446213, 0.063*\"chang\" + 0.052*\"flood\" + 0.050*\"australia\" + 0.049*\"work\" + 0.047*\"trial\"\n",
      "1, 0.016667168587446213, 0.046*\"time\" + 0.044*\"return\" + 0.044*\"storm\" + 0.041*\"rise\" + 0.039*\"train\"\n",
      "2, 0.016667168587446213, 0.055*\"coast\" + 0.043*\"north\" + 0.043*\"fight\" + 0.042*\"gold\" + 0.035*\"break\"\n",
      "4, 0.016667168587446213, 0.056*\"end\" + 0.039*\"queensland\" + 0.038*\"review\" + 0.037*\"join\" + 0.036*\"green\"\n",
      "5, 0.016667168587446213, 0.094*\"council\" + 0.043*\"union\" + 0.030*\"reject\" + 0.029*\"look\" + 0.026*\"free\"\n",
      "6, 0.016667168587446213, 0.119*\"say\" + 0.094*\"plan\" + 0.033*\"cut\" + 0.033*\"research\" + 0.026*\"state\"\n",
      "7, 0.016667168587446213, 0.074*\"face\" + 0.049*\"lead\" + 0.044*\"court\" + 0.044*\"dead\" + 0.040*\"law\"\n",
      "8, 0.016667168587446213, 0.063*\"report\" + 0.062*\"open\" + 0.055*\"school\" + 0.043*\"nation\" + 0.029*\"close\"\n",
      "9, 0.016667168587446213, 0.064*\"urg\" + 0.049*\"boost\" + 0.046*\"farmer\" + 0.032*\"weather\" + 0.031*\"famili\"\n",
      "10, 0.016667168587446213, 0.081*\"warn\" + 0.076*\"hous\" + 0.045*\"fear\" + 0.040*\"price\" + 0.031*\"public\"\n",
      "11, 0.016667168587446213, 0.098*\"win\" + 0.043*\"world\" + 0.043*\"cup\" + 0.033*\"final\" + 0.024*\"unit\"\n",
      "12, 0.016667168587446213, 0.103*\"interview\" + 0.065*\"sydney\" + 0.049*\"take\" + 0.038*\"indigen\" + 0.037*\"cost\"\n",
      "14, 0.016667168587446213, 0.059*\"fund\" + 0.049*\"arrest\" + 0.038*\"offer\" + 0.031*\"get\" + 0.030*\"drug\"\n",
      "16, 0.016667168587446213, 0.048*\"ban\" + 0.041*\"fall\" + 0.035*\"home\" + 0.034*\"start\" + 0.033*\"labor\"\n",
      "17, 0.016667168587446213, 0.081*\"govt\" + 0.052*\"govern\" + 0.039*\"group\" + 0.038*\"protest\" + 0.033*\"tell\"\n",
      "18, 0.016667168587446213, 0.077*\"death\" + 0.047*\"minist\" + 0.040*\"china\" + 0.036*\"probe\" + 0.036*\"support\"\n",
      "19, 0.016667168587446213, 0.060*\"year\" + 0.057*\"jail\" + 0.052*\"kill\" + 0.050*\"help\" + 0.041*\"appeal\"\n"
     ]
    }
   ],
   "source": [
    "# sort per the value (second element in the tuple)\n",
    "for index, score in sorted(lda_model[bow_corpus[8]], key=lambda x: x[1], reverse=True): \n",
    "    print(f\"{index}, {score}, {lda_model.print_topic(index, 5)}\") # only print out the first 5 words in the topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Bag-of-Words model, topic #9 is close to the true headline. We see there are `farmer` and `weather` in the predicted topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. TF-IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 0.6833289265632629, 0.046*\"time\" + 0.044*\"return\" + 0.044*\"storm\" + 0.041*\"rise\" + 0.039*\"train\"\n",
      "12, 0.016666902229189873, 0.103*\"interview\" + 0.065*\"sydney\" + 0.049*\"take\" + 0.038*\"indigen\" + 0.037*\"cost\"\n",
      "0, 0.016666898503899574, 0.063*\"chang\" + 0.052*\"flood\" + 0.050*\"australia\" + 0.049*\"work\" + 0.047*\"trial\"\n",
      "2, 0.016666898503899574, 0.055*\"coast\" + 0.043*\"north\" + 0.043*\"fight\" + 0.042*\"gold\" + 0.035*\"break\"\n",
      "3, 0.016666898503899574, 0.103*\"new\" + 0.074*\"nsw\" + 0.068*\"australian\" + 0.059*\"day\" + 0.049*\"test\"\n",
      "4, 0.016666898503899574, 0.056*\"end\" + 0.039*\"queensland\" + 0.038*\"review\" + 0.037*\"join\" + 0.036*\"green\"\n",
      "5, 0.016666898503899574, 0.094*\"council\" + 0.043*\"union\" + 0.030*\"reject\" + 0.029*\"look\" + 0.026*\"free\"\n",
      "6, 0.016666898503899574, 0.119*\"say\" + 0.094*\"plan\" + 0.033*\"cut\" + 0.033*\"research\" + 0.026*\"state\"\n",
      "7, 0.016666898503899574, 0.074*\"face\" + 0.049*\"lead\" + 0.044*\"court\" + 0.044*\"dead\" + 0.040*\"law\"\n",
      "8, 0.016666898503899574, 0.063*\"report\" + 0.062*\"open\" + 0.055*\"school\" + 0.043*\"nation\" + 0.029*\"close\"\n",
      "9, 0.016666898503899574, 0.064*\"urg\" + 0.049*\"boost\" + 0.046*\"farmer\" + 0.032*\"weather\" + 0.031*\"famili\"\n",
      "10, 0.016666898503899574, 0.081*\"warn\" + 0.076*\"hous\" + 0.045*\"fear\" + 0.040*\"price\" + 0.031*\"public\"\n",
      "11, 0.016666898503899574, 0.098*\"win\" + 0.043*\"world\" + 0.043*\"cup\" + 0.033*\"final\" + 0.024*\"unit\"\n",
      "13, 0.016666898503899574, 0.141*\"man\" + 0.069*\"charg\" + 0.059*\"attack\" + 0.050*\"crash\" + 0.048*\"car\"\n",
      "14, 0.016666898503899574, 0.059*\"fund\" + 0.049*\"arrest\" + 0.038*\"offer\" + 0.031*\"get\" + 0.030*\"drug\"\n",
      "15, 0.016666898503899574, 0.201*\"polic\" + 0.049*\"set\" + 0.041*\"feder\" + 0.039*\"alleg\" + 0.038*\"defend\"\n",
      "16, 0.016666898503899574, 0.048*\"ban\" + 0.041*\"fall\" + 0.035*\"home\" + 0.034*\"start\" + 0.033*\"labor\"\n",
      "17, 0.016666898503899574, 0.081*\"govt\" + 0.052*\"govern\" + 0.039*\"group\" + 0.038*\"protest\" + 0.033*\"tell\"\n",
      "18, 0.016666898503899574, 0.077*\"death\" + 0.047*\"minist\" + 0.040*\"china\" + 0.036*\"probe\" + 0.036*\"support\"\n",
      "19, 0.016666898503899574, 0.060*\"year\" + 0.057*\"jail\" + 0.052*\"kill\" + 0.050*\"help\" + 0.041*\"appeal\"\n"
     ]
    }
   ],
   "source": [
    "# test the same doc on tf-idf model\n",
    "\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[8]], key=lambda x: x[1], reverse=True): \n",
    "    print(f\"{index}, {score}, {lda_model.print_topic(index, 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf-idf performs similar to Bag-of-Words model. We see `farmer`, `weater` also appear in predicted topic #9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing \n",
    "\n",
    "Time to test our model on unseen documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(74, 1), (182, 1), (274, 1), (548, 1), (753, 1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unseen_doc = \"US closes 5 military bases in Afghanistan as part of Taliban peace deal\"\n",
    "bow_vector = dictionary.doc2bow(text_preprocess(unseen_doc))\n",
    "bow_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19, 0.6749829649925232, 0.060*\"year\" + 0.057*\"jail\" + 0.052*\"kill\" + 0.050*\"help\" + 0.041*\"appeal\"\n",
      "8, 0.17500627040863037, 0.063*\"report\" + 0.062*\"open\" + 0.055*\"school\" + 0.043*\"nation\" + 0.029*\"close\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_vector], key=lambda x: x[1], reverse=True): # sort by 2nd element in the tuple\n",
    "    print(f\"{index}, {score}, {lda_model.print_topic(index, 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall topic #19 is about these words, which is about voilence.\n",
    "\n",
    "0.050*\"jail\" + 0.049*\"year\" + 0.032*\"hold\" + 0.030*\"injur\" + 0.028*\"teen\" + 0.028*\"abus\" + 0.028*\"drug\" + 0.027*\"strike\" + 0.027*\"fire\" + 0.027*\"show\"\n",
    "\n",
    "The prediction about this headline is topic #19, which is not too bad."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
